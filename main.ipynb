{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import TransformChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './Dataset/instagram.csv'\n",
    "K_RETRIEVER_VALUE = 5\n",
    "\n",
    "openai_embedding = OpenAIEmbeddings()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0) \n",
    "\n",
    "query = \"What are the specific features or aspects that users appreciate the most in our application?\"\n",
    "# # query = \"What are the primary reasons users express dissatisfaction with Instagram?\"\n",
    "# # query = \"Can you identify emerging trends or patterns in recent user reviews that may impact our product strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH)[:5000]\n",
    "dict_data = df.to_dict(orient=\"records\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)\n",
    "print(df['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.isna().sum())\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['review_description']\n",
    "token_count = []\n",
    "encoding = tiktoken.get_encoding('cl100k_base')\n",
    "max_token = 0\n",
    "for i in data:\n",
    "    num_tokens = len(encoding.encode(i))\n",
    "    if max_token < num_tokens:\n",
    "        max_token = num_tokens\n",
    "    token_count.append(num_tokens)\n",
    "    # print(num_tokens)\n",
    "print(f\"Highest Token Count : {max_token}\")\n",
    "plt.hist(token_count, bins=100, color='cyan', edgecolor='black')\n",
    "plt.title('Token Count Distribution')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=item[\"review_description\"],\n",
    "        metadata={\"rating\": item[\"rating\"], \"review_date\": item[\"review_date\"]}\n",
    "    )\n",
    "    for item in dict_data\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=120,  \n",
    "    chunk_overlap=10  \n",
    ")\n",
    "\n",
    "print(\"Splitting...\")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(\"Storing...\")\n",
    "vector_store = Chroma.from_documents(documents=documents, embedding=openai_embedding)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": K_RETRIEVER_VALUE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question : {query}\")\n",
    "\n",
    "results = retriever.get_relevant_documents(query)\n",
    "for result in results:\n",
    "    print(f\"Review Chunk: {result.page_content}\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n",
    "    print()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Please focus on the clarity of the question and add more details to it. Provide these alternative questions separated by newlines. Original question: {query}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "print(generate_queries.invoke({\"query\":query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"review_date\",\n",
    "        description=\"The time when the review was submitted\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", \n",
    "        description=\"A user rating scale ranging from 1 to 5, where 1 indicates poor quality and 5 represents excellent quality\", \n",
    "        type=\"float\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"User rating of an application\"\n",
    "\n",
    "self_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")\n",
    "\n",
    "self_retriver_result = self_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(self_retriver_result)\n",
    "print(len(self_retriver_result))\n",
    "\n",
    "for i in self_retriver_result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_unique(documents):\n",
    "    flattened = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "def process_multiple_queries(inputs):\n",
    "    # print(type(inputs))\n",
    "    # print(inputs)\n",
    "    queries = inputs[\"query\"]\n",
    "    retrieval_results = [self_retriever.invoke(query) for query in queries]\n",
    "    # print(retrieval_results)\n",
    "    unique_docs = get_only_unique(retrieval_results)\n",
    "    \n",
    "    return {\"documents\": unique_docs}\n",
    "\n",
    "\n",
    "queries = generate_queries.invoke({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"query\"],  \n",
    "    output_variables=[\"documents\"],  \n",
    "    transform=process_multiple_queries \n",
    ")\n",
    "docs = retrieval_chain.invoke({\"query\": queries})\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = docs['documents']\n",
    "print(len(test))\n",
    "for i in test:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI assistant for question-answering tasks. Use the following pieces of retrieved context and information to answer the question. If you don't know the answer, say that you don't know. If the data is not relevant to the question, don't use the data. Context: {context} Question: {query}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generation_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# print(generation_chain)\n",
    "generation_chain.invoke({\"query\": query, \"context\":docs['documents']})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
