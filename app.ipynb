{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import TransformChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting...\n",
      "Storing...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = './Dataset/instagram.csv'\n",
    "K_RETRIEVER_VALUE = 5\n",
    "\n",
    "openai_embedding = OpenAIEmbeddings()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0) \n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)[:10000]\n",
    "dict_data = df.to_dict(orient=\"records\")\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=item[\"review_description\"],\n",
    "        metadata={\"rating\": item[\"rating\"], \"review_date\": item[\"review_date\"]}\n",
    "    )\n",
    "    for item in dict_data\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=120,  \n",
    "    chunk_overlap=10  \n",
    ")\n",
    "\n",
    "print(\"Splitting...\")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(\"Storing...\")\n",
    "vector_store = Chroma.from_documents(documents=documents, embedding=openai_embedding)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": K_RETRIEVER_VALUE})\n",
    "print(\"Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Please focus on the clarity of the question and add more details to it. Provide these alternative questions separated by newlines. Original question: {query}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "            prompt_perspectives \n",
    "            | llm\n",
    "            | StrOutputParser() \n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "\n",
    "metadata_field_info = [\n",
    "    \n",
    "    AttributeInfo(\n",
    "        name=\"rating\", \n",
    "        description=\"A user rating scale ranging from 1 to 5, where 1 indicates poor quality and 5 represents excellent quality\", \n",
    "        type=\"float\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"User rating of an application\"\n",
    "\n",
    "self_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_unique(documents):\n",
    "    flattened = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "def process_multiple_queries(inputs):\n",
    "    queries = inputs[\"query\"]\n",
    "    retrieval_results = [self_retriever.invoke(query) for query in queries]\n",
    "    unique_docs = get_only_unique(retrieval_results)\n",
    "    \n",
    "    return {\"documents\": unique_docs}\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"query\"],  \n",
    "    output_variables=[\"documents\"],  \n",
    "    transform=process_multiple_queries \n",
    ")\n",
    "\n",
    "template = \"\"\"You are an AI assistant for question-answering tasks. Use the following pieces of retrieved context and information to answer the question. If you don't know the answer, say that you don't know. If the data is not relevant to the question, don't use the data. Put newline symbol if necessary. Context: {context} Question: {query}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generation_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__name__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5050\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [13/Jan/2025 03:39:41] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:39:41] \"GET /static/js/script.js HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:39:41] \"GET /static/styles/index.css HTTP/1.1\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Translation Length : 5\n",
      "Relevant docs length : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Jan/2025 03:39:54] \"GET /?prompt=What+are+the+specific+features+or+aspects+that+users+appreciate+the+most+in+our+application? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:39:54] \"GET /static/styles/index.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:39:54] \"GET /static/js/script.js HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Translation Length : 5\n",
      "Relevant docs length : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Jan/2025 03:40:10] \"GET /?prompt=What+are+the+specific+features+or+aspects+that+users+appreciate+the+most+in+our+application?+Make+it+in+points! HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:40:10] \"GET /static/styles/index.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:40:10] \"GET /static/js/script.js HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Translation Length : 5\n",
      "Relevant docs length : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Jan/2025 03:40:31] \"GET /?prompt=What+are+the+primary+reasons+users+express+dissatisfaction+with+Instagram? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:40:31] \"GET /static/styles/index.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:40:31] \"GET /static/js/script.js HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Translation Length : 5\n",
      "Relevant docs length : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Jan/2025 03:40:48] \"GET /?prompt=Can+you+identify+emerging+trends+or+patterns+in+recent+user+reviews+that+may+impact+our+product+strategy? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:40:48] \"GET /static/styles/index.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Jan/2025 03:40:48] \"GET /static/js/script.js HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, url_for,redirect, render_template, Response,request\n",
    "from flask import Flask\n",
    "app = Flask('__name__')\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    global generate_queries, self_retriever, retrieval_chain, generation_chain\n",
    "    prompt = request.args.get('prompt', '') \n",
    "\n",
    "    result = \"\"\n",
    "    if prompt != '':\n",
    "        prompt = prompt\n",
    "        queries = generate_queries.invoke({\"query\":prompt})\n",
    "        print(f\"Query Translation Length : {len(queries)}\")\n",
    "        retrieval_chain = TransformChain(\n",
    "            input_variables=[\"query\"],  \n",
    "            output_variables=[\"documents\"],  \n",
    "            transform=process_multiple_queries \n",
    "        )\n",
    "        docs = retrieval_chain.invoke({\"query\": queries})\n",
    "        print(\"Relevant docs length : \"+ str(len(docs['documents'])))\n",
    "        result = generation_chain.invoke({\"query\": prompt, \"context\":docs['documents']})\n",
    "        # print(result)\n",
    "        result = result.replace('\\n', '<br>')\n",
    "\n",
    "\n",
    "    \n",
    "    return render_template('index.html', prompt=prompt, result=result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5050,debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
